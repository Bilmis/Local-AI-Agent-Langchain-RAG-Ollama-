# LocalAIAgentWithRAG

This project demonstrates how to build a **local AI-powered agent** capable of retrieving and reasoning over custom data using **LangChain**, **RAG (Retrieval-Augmented Generation)**, and **Ollama** as the local LLM backend.

## üöÄ Features
- **Local-first**: No dependency on external APIs for inference.
- **RAG Pipeline**: Retrieve relevant context from documents before generating responses.
- **LangChain Integration**: Orchestrates prompts, retrieval, and LLM responses.
- **Customizable Data Source**: Easily plug in your own documents.
- **Modular Design**: Easy to extend with more tools and agents.

## üõ†Ô∏è Tech Stack
- **[LangChain](https://www.langchain.com/)** ‚Äì for prompt orchestration & chaining.
- **[Ollama](https://ollama.ai/)** ‚Äì run LLMs locally.
- **[Python](https://www.python.org/)** ‚Äì backend scripting and orchestration.
- **Vector Database** (e.g., FAISS, Chroma) ‚Äì for storing document embeddings.

## ‚ö° Getting Started

### 1Ô∏è‚É£ Prerequisites
- Install **Python 3.9+**
- Install [Ollama](https://ollama.ai/download) locally
- Install project dependencies: pip install -r requirements.txt

### 2Ô∏è‚É£ Pull a Model with Ollama (example: mistral model)
ollama pull mistral 

### 3Ô∏è‚É£ Run the Agent
python main.py

## How it Works

1. Document Ingestion ‚Äì Load your local files.
2. Embedding & Storage ‚Äì Convert text to embeddings and store in a vector DB.
3. Query Processing ‚Äì On a user query, retrieve the most relevant chunks.
4. LLM Response ‚Äì Pass the retrieved chunks + query to the LLM via LangChain.

## Future Improvements
Web-based UI
More LLM options
Multi-document & multi-modal support
Agent tools for executing code or web search